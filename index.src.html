<h1>Self-Review Questionnaire: Security and Privacy</h1>
<pre class="metadata">
Status: DREAM
ED: https://w3ctag.github.io/security-questionnaire/
Shortname: security-privacy-questionnaire
Level: 1
Editor: Lukasz Olejnik, Independent researcher, me@lukaszolejnik.com
Former Editor: Mike West, Google Inc., mkwst@google.com
Group: tag
Indent: 2
Abstract:
  This document provides a points to help in considering the privacy impact of
  a new feature or specification as well as common mitigation strategies for
  common privacy impacts. The questions are meant to be useful when considering
  the security and privacy aspects of a new feature or specification and the
  mitigation strategies are meant to assist in the design of the feature or
  specification. Authors of a new proposal or feature should implement the
  mitigations as appropriate; doing so will assist in addressing the respective
  points in the questionnaire. Given the variety and nature of specifications,
  it is likely that the listed questions will not be comprehensive in a way
  enabling to reason about the full privacy impact, and some mitigations may
  not be appropriate or other mitigations may be necessary. It is nonetheless
  the aim to present the questions and mitigations as a starting point, helping
  to consider security and privacy at the start of work on a new feature, and
  throughout the lifecycle of a feature.

  It is not meant as a "security checklist", nor does an editor or group’s use
  of this questionnaire obviate the editor or group’s responsibility to obtain
  "wide review" of a specification security and privacy properties before
  publication. Furthermore, the filled questionnaire should not be understood
  as security and privacy considerations, although part of the answers may be
  relevant in drafting the considerations.

Version History: https://github.com/w3ctag/security-questionnaire/commits/master/index.src.html
!Bug Reports: <a href="https://github.com/w3ctag/security-questionnaire/issues/new">via the w3ctag/security-questionnaire repository on GitHub</a>
</pre>

<!-- Big Text: Intro -->
<section>
  <h2 id="intro">Introduction</h2>

  New features make the web a stronger and livelier platform.  Throughout the
  feature development process — planning, drafting, and implementing — there
  are both foreseeable and unexpected security and privacy risks.  These risks
  may arise from the nature of the feature, some of its part(s), or unforeseen
  interactions with other features and may be mitigated through careful design
  and consideration of security and privacy design patterns.

  Standardizing web features presents unique challenges as, inherently, the web
  standardization process requires multiple independent implementations of a
  feature.  As a result, descriptions, protocols and algorithms need to be
  considered strictly so they are broadly adoption by vendors with large user
  bases.

  A user agent is the user’s agent, and it is important that feature authors
  consider that if their feature contains privacy risks, then, user agents may
  implement the feature in a compatibility breaking way to protect user
  privacy.  This is especially true if and when features are not clearly
  defined and privacy exposures are identified and left unmitigated — or left
  to user agents to mitigate — vendors may implement their own compatibility
  breaking mitigations that risk wide adoption of the feature.

  This is why each Working Group needs to consider <em>security and
  privacy</em> <b>by default</b>. This consideration is <b>mandatory</b>.
  Furthermore, assessing the impact of a mechanism on privacy should be done
  from the ground up, during each iteration of the specification. The
  questionnaire and guidance presented here is meant to assist authors consider
  security and privacy in their feature as earlier as possible — beginning with
  ideation in Community Groups through to formalization in Working Groups.
  The guidance should help authors develop standards with privacy by default
  allowing for the questionnaire to be addressed simply and straightforwardly.

  Providing adequate and informative answers to the questions, such as
  providing context, or explaining the reasoning behind certain choices, will
  also help in a wide security and privacy review performed later on, within
  the W3C review proces, and beyond.

  This document encourages early review by posing a number of questions that
  you as an individual reader/writer/contributor of a specification can ask —
  and that working groups and spec editors need to consider, prior asking for a
  more formal review. The intent is to highlight areas which have historically
  had interesting implications on a user’s security or privacy, and thereby to
  focus the editor, working group attention, and reviewers' attention on areas
  that might previously have been overlooked.

  This document does not attempt to define what privacy is (in a Web context).
  Instead privacy is the sum of what is contained in this document. While this
  may not be exactly what most readers would typically assume but privacy is a
  complicated concept with a rich history that spans many disciplines and there
  remains confusion over the meaning.</p>

  The audience of this document is general:

  *   The editors and contributors who are responsible for the development of
      the feature,
  *   The W3C TAG, who receive the questionnaire along with the request, and in
      line with the W3C Process,
  *   External audience (developers, designers, etc.) wanting to understand the
      possible security and privacy implications.

<h2 id="howtouse">How To Use The Questionnaire</h2>

  Thinking about security and privacy risks and mitigations early in a project
  is the best approach as it helps ensure the privacy of your feature at an
  architectural level and ensures the result, descriptions, protocols and
  algorithms incorporate privacy by default as opposed to through possible
  implementation mitigations.

  The Privacy Interest Group (PING) recommends that a feature group review the
  guidance and questionnaire when first considering their feature and meet with
  PING at that time to discuss any questions they have about how the
  guidance/questionnaire intersects with their feature at a conceptual level.
  After the feature group has developed their feature with the
  guidance/questionnaire informing their development process, the group should
  bring an early draft of their feature specification with Privacy consideration
  section to PING for review.  From there the feature group should iterate on
  their design.

  When requesting a Technical Architecture Group review, include the filled
  questionnaire, along with the description of changes or observations made
  during the design process. This allows external reviewers understand the
  rationale, as well as the challenges and evolution of the feature, with
  respect to security and privacy.

  It is understandable that developers may not always have the necessary data
  to see the broader picture and possible implications, for example in relation
  to other existing web functionalities. The answers to the questionnaire are
  meant as help and input for people who may nonetheless make security and
  privacy remarks, or the assessment.

</section>

<!-- Big Text: Threats -->
<section>
  <h2 id="threats">Threat Models</h2>

  To consider security and privacy it is convenient to think in terms of threat
  models, a way to illuminate the possible risks.

  There are some concrete privacy concerns that should be considered when
  developing a feature for the web platform:
  *   Surveillance: Surveillance is the observation or monitoring of an
      individual's communications or activities.
  *   Stored Data Compromise: End systems that do not take adequate measures to
      secure stored data from unauthorized or inappropriate access.
  *   Intrusion: Intrusion consists of invasive acts that disturb or interrupt
      one's life or activities.
  *   Misattribution: Misattribution occurs when data or communications related
      to one individual are attributed to another.
  *   Correlation: Correlation is the combination of various pieces of
      information related to an individual or that obtain that characteristic
      when combined.
  *   Identification: Identification is the linking of information to a
      particular individual to infer an individual's identity or to allow the
      inference of an individual's identity.
  *   Secondary Use: Secondary use is the use of collected information about an
      individual without the individual's consent for a purpose different from
      that for which the information was collected.
  *   Disclosure: Disclosure is the revelation of information about an
      individual that affects the way others judge the individual.
  *   Exclusion: Exclusion is the failure to allow individuals to know about
      the data that others have about them and to participate in its handling
      and use.

  In the mitigations section, this document outlines a number of techniques
  that can be applied to mitigate these risks.

  Enumerated below are some broad classes of threats that should be
  considered when developing a web feature.

  <h3 id="passive-network">Passive Network Attackers</h3>

  A <dfn>passive network attacker</dfn> has read-access to the bits going over
  the wire between users and the servers they're communicating with. She can't
  <em>modify</em> the bytes, but she can collect and analyze them.

  Due to the decentralized nature of the internet, and the general level of
  interest in user activity, it's reasonable to assume that practically every
  unencrypted bit that's bouncing around the network of proxies, routers, and
  servers you're using right now is being read by someone. It's equally likely
  that some of these attackers are doing their best to understand the encrypted
  bits as well, including storing encrypted communications for later
  cryptanalysis (though that requires significantly more effort).

  *   The IETF's "Pervasive Monitoring Is an Attack" document [[RFC7258]] is
      useful reading, outlining some of the impacts on privacy that this
      assumption entails.

  *   Governments aren't the only concern; your local coffee shop is likely to
      be gathering information on its customers, your ISP at home is likely to
      be doing the same.

  <h3 id="active-network">Active Network Attackers</h3>

  An <dfn>active network attacker</dfn> has both read- and write-access to the
  bits going over the wire between users and the servers they're communicating
  with. She can collect and analyze data, but also modify it in-flight,
  injecting and manipulating Javascript, HTML, and other content at will.
  This is more common than you might expect, for both benign and malicious
  purposes:

  *   ISPs and caching proxies regularly cache and compress images before
      delivering them to users in an effort to reduce data usage. This can be
      especially useful for users on low-bandwidth, high-latency devices like
      phones.

  *   ISPs also regularly inject JavaScript [[COMCAST]] and other identifiers
      [[VERIZON]] for less benign purposes.

  *   If your ISP is willing to modify substantial amounts of traffic flowing
      through it for profit, it's difficult to believe that state-level
      attackers will remain passive.

  <h3 id="sop-violations">Same-Origin Policy Violations</h3>

  The <dfn>same-origin policy</dfn> is the cornerstone of security on the web;
  one origin should not have direct access to another origin's data (the policy
  is more formally defined in Section 3 of [[RFC6454]]). A corollary to this
  policy is that an origin should not have direct access to data that isn't
  associated with <em>any</em> origin: the contents of a user's hard drive,
  for instance. Various kinds of attacks bypass this protection in one way or
  another. For example:

  *   <dfn local-lt="XSS">Cross-site scripting attacks</dfn> involve an
      attacker tricking an origin into executing attacker-controlled code in
      the context of a target origin.

  *   <dfn local-lt="CSRF">Cross-site request forgery attacks</dfn> trick
      user agents into exerting a user's ambient authority on sites where
      they've logged in by submitting requests on their behalf.

  *   Data leakage occurs when bits of information are inadvertantly made
      available cross-origin, either explicitly via CORS headers [[CORS]],
      or implicitly, via side-channel attacks like [[TIMING]].

  <h3 id="third-party-tracking">Third-Party Tracking</h3>

  Part of the power of the web is its ability for a page to pull in content
  from other third parties — from images to javascript — to enhance the content
  and/or a user's experience of the site.  However, when a page pulls in
  content from third parities, it inherently leaks some information to third
  parties — referer information and other information that may be used to track
  and profile a user.  This includes the fact that cookies go back to the
  domain that initially stored them allowing for cross origin tracking.
  Moreover, third parties can gain execution power through third party
  Javascript being included by a webpage.  While pages can take steps to
  mitigate the risks of third party content and browsers may differentiate
  how they treat first and third party content from a given page, the risk of
  new functionality being executed by third parties rather than the first party
  site should be considered in the feature development process.

  *   The simplest example is injecting a link to a site that behaves
      differently under specific condition, for example based on the fact that
      user is or is not logged to the site. This may reveal that the user has
      an account on a site.

  <h3 id="legitimate-misuse">Legitimate Misuse</h3>

  Even when powerful features are made available to developers, it does not
  mean that all the uses should always be a good idea, or justified; in fact,
  data privacy regulations around the world may even put limits on certain uses
  of data. In the context of first party, a legitimate website is potentially
  able to interact with powerful features to learn about the user behavior or
  habits. For example:

  *   Tracking the user while browsing the website via mechanisms such as mouse
      move tracking

  *   Behavioral profiling of the user based on the usage patterns

  *   Accessing powerful features enabling to reason about the user system,
      himself or the user surrounding, such as a webcam, Web Bluetooth or
      sensors

  This point is admittedly different from others - and underlines that even if
  something may be possible, it does not mean it should always be done,
  including the need for considering a privacy impact assessment or even an
  ethical assessment. When designing a specification with security and privacy
  in mind, all both use and misuse cases should be in scope.


</section>

<!-- Big Text: Questions -->
<section>
  <h2 id="questions">Questions to Consider</h2>

  <h3 id="pii">
    How does the specification deal with personal information allowing to single out the user?
  </h3>

    Personal data are information relating to individuals. Singling out the user is possible when some unique
    information or traits are collected. For example, personal data allow singling out individuals on its own,
    or in combination with other information, to identify a specific person. The exact definition of what is
    considered PII varies around the world and is rapidly evolving, but may include, among the others,
    things like a home address, an email address, birthdates, usernames, fingerprints, biometric data,
    health information, but also identifiers such as cookies, identifiers, IP address, health status, preferences,
    affinities, beliefs, and much more, depending on the context.

    If the specification under consideration exposes personal data to the web,
    it’s important to consider ways to mitigate the impacts. For instance:

    *   A feature which uses biometric data (fingerprints or retina scans) should refuse to expose the raw data to the web, instead using the raw data only to unlock some origin-specific and ephemeral secret and transmitting that secret instead.
    *   Including a factor of user mediation should be considered, in order to ensure that no data is exposed without a user’s explicit choice (and hopefully understanding). One way to achieve this may be the use of Permission API [PERMISSIONS], or additional dialogs like in Payment Request API [PAYMENT-REQUEST-API]
    *   The users should be made aware if and when their private data may be exposed to the web

  If the specification deals with or introduces identifiers (persistent or not), the process should be documented,
   along with the nature of the identifier and its lifetime, whether it is limited to a particular origin or not.


  <h3 id="credentials">
    How does this specification deal with high-value data?
  </h3>

  Data which isn't <a>personally-identifiable</a> can still be quite valuable.
  Sign-in credentials (like username/password pairs, or OAuth refresh tokens)
  can be extrememly powerful in the wrong hands, as can financial instruments
  like credit card data. Making this data available to JavaScript, for instance,
  could expose it to <a>XSS</a> attacks and <a>active network attackers</a> who
  could inject code to read and exfiltrate the data. For instance:

  *   Credential Management [[CREDENTIAL-MANAGEMENT]] allows sites to request
      a user's credentials from a user agent's password manager in order to
      sign the user in quickly and easily. This opens the door for abuse, as
      a single XSS vulnerability could expose user data trivially to
      JavaScript. The Credential Managerment API mitigates
      the risk by offering the username and password as only an opaque
      <code>FormData</code> object which cannot be directly read by JavaScript
      and strongly suggests that authors use Content Security Policy [[CSP]] with
      resonable <code>connect-src</code> and <code>form-action</code> values to
      further mitigate the risk of exfiltration.

  <h3 id="persistent-origin-specific-state">
    Might this specification introduce new state for an origin that persists across browsing sessions?
  </h3>

  For example:

  *   Service Worker [[SERVICE-WORKERS]] intercept all requests made by an
      origin, allowing sites to function perfectly even when offline. A
      maliciously-injected service worker, however, would be devastating (as
      documented in that spec's
      <a href="http://www.w3.org/TR/service-workers/#security-considerations">security
      considerations section</a>). They mitigate the risks an <a>active network
      attacker</a> or <a>XSS</a> vulnerability present by requiring an
      encrypted and authenticated connection in order to register a service
      worker.

  *   Platform-specific DRM implementations might expose origin-specific
      information in order to help identify users and determine whether they
      ought to be granted access to a specific piece of media. These kinds of
      identifiers should be carefully evaluated to determine how abuse can be
      mitigated; identifiers which a user cannot easily change are very
      valuable from a tracking perspective, and protecting the identifiers from
      an active network attacker is an important concern.

  *   Cookies, <code>ETag</code>, <code>Last Modified</code>, <code>Local
      Storage</code>, <code>Indexed DB</code>, etc. all allow an origin to
      store information about a user, and retrieve it later, directly or
      indirectly. User agents mitigate the risk that these kinds of storage
      mechanisms will form a persistent identifier by offering users the
      ability to wipe out the data contained in these types of storage.

  *   Fingerprinting is a technique establishing a unique identifier based on the capabilities of specific
  feature alone (e.g. Canvas), or combining a number of features. Specifications and user agents should treat
  the risk of fingerprinting by carefully considering the surface of available information, and the relative
  differences between software and hardware stacks. Sometimes reducing fingerprintability may be revolve to
  specific operational considerations, for example providing information in same order (i.e. list of fonts),
  but sometimes this is not so simple.

  *   Some readout information that is subject to change may still act as a short-term identifier,
  and possibly introduce a risk of misuse (examples: <a href="https://eprint.iacr.org/2015/616">Leaking Battery</a>,
  <a href="https://blog.lukaszolejnik.com/battery-status-not-included-assessing-privacy-in-w3c-web-standards/">Battery Status Not Included</a>).
  Good example may be the readout of ambient light level [AMBIENT-LIGHT], or battery [BATTERY-STATUS-API]


  <h3 id="persistent-identifiers">
    Does this specification expose persistent, cross-origin state to the web?
  </h3>

  For example:

  *   The <code>GL_RENDERER</code> string exposed by some WebGL implementations
      improves performance in some kinds of applications, but does so at the
      cost of adding persistent state to a user's fingerprint. These kinds of
      device-level details should be carefully weighed to ensure that the costs
      are outweighed by the benefits.

  *   The {{NavigatorPlugins}} list exposed via the DOM practically never
      changes for most users. Some user agents have taken steps to reduce the
      entropy introduced by
      <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=757726">disallowing
      direct enumeration of the plugin list</a>.

  *   The unexpected cases such as the one of [BATTERY-STATUS-API] when the battery level allowed to
      reason about the capacity as provided by the operating system


  <h3 id="other-data">
    Does this specification expose any other data to an origin that it doesn't
    currently have access to?
  </h3>

  As noted above in [[#sop-violations]], the <a>same-origin policy</a> is an
  important security barrier that new features need to carefully consider.
  If a specification exposes details about another origin's state, or allows
  POST or GET requests to be made to another origin, the consequences can be
  severe.

  *   Content Security Policy [[CSP]] unintentionally exposed redirect targets
      cross-origin by allowing one origin to infer details about another origin
      through violation reports (see [[HOMAKOV]]). The working group eventually
      mitigated the risk by reducing a policy's granularity after a redirect.

  *   Beacon [[BEACON]] allows an origin to send POST requests to an endpoint
      on another origin. They decided that this feature didn't add any new
      attack surface above and beyond what normal form submission entails, so
      no extra mitigation was necessary.

  <h3 id="string-to-script">
    Does this specification enable new script execution/loading mechanisms?
  </h3>

  * HTML Imports [[HTML-IMPORTS]] create a new script-loading mechanism, using
    <{link}> rather than <{script}>, which might be easy to overlook when
    evaluating an application's attack surface. The working group notes this
    risk, and ensured that they required reasonable interactions with Content
    Security Policy's <code>script-src</code> directive.

  * New string-to-script mechanism? (e.g. `eval()` or `setTimeout([string], ...)`)

  * What about style?

  <h3 id="location">Does this specification allow an origin access to a user's location?</h3>

  A user's location is highly-desirable information for a variety of use cases.
  It is also, understandably, information which many users are reluctant to
  share, as it can be both highly identifying, and potentially creepy. New
  features which make use of geolocation information, or which expose it to the
  web in new ways should carefully consider the ways in which the risks of
  unfettered access to a user's location could be mitigated. For instance:

  *   Geolocation information can serve many use cases at a much less granular
      precision than the user agent can offer. For instance, a resturaunt
      recommendation can be generated by asking for a user's city-level location
      rather than a position accurate to the centimeter.

  *   A recent Geofencing proposal [[GEOFENCING]] ties itself to service workers
      and therefore to encrypted and authenticated origins.

  <h3 id="sensors">Does this specification allow an origin access to sensors on a user's device?</h3>

    Powerful features allowing to query information about the user system or environment may open new interesting
    use cases, as well as expanding risks and changing threat models.
    Examples of new feature of the kind may include sensors, communication channels such as Bluetooth or USB.

  <h3 id="local-device">Does this specification allow an origin access to aspects of a user's local computing environment?</h3>

    Features enabling to modify or query screen sizes or installed fonts may be useful but in some contexts might be introducing possible risks. Additionally, functionality facilitating access reason about the user computing environment such as by means of bluetooth or USB should be accounted for, whether the employed identifiers are long term or not.
  *   [AMBIENT-LIGHT] and [GENERIC-SENSORS] have an extensive discussion around the security and privacy risks


  <h3 id="remote-device">
    Does this specification allow an origin access to other devices?
  </h3>

  Accessing other devices, both via network connections and via
      direct connection to the user's machine (e.g. via Bluetooth,
      NFC, or USB), could expose vulnerabilities - some of
      these devices were not created with web connectivity in mind and may be inadequately
      hardened against malicious input, or with the use on the web.

  *   The Network Service Discovery API [[DISCOVERY]] recommends CORS preflights
      before granting access to a device, and requires user agents to involve
      the user with a permission request of some kind. The spec's
      <a href="https://dvcs.w3.org/hg/dap/raw-file/tip/discovery-api/Overview.html#security-and-privacy-considerations">Security
      and privacy considerations"</a> section has more details.

  *   Likewise, the Web Bluetooth [[BLUETOOTH]] has an extensive discussion of
      <a href="https://webbluetoothcg.github.io/web-bluetooth/#security-and-privacy-considerations">"Security
      and privacy considerations"</a>, which is worth reading as an example for
      similar work.

      Direct connections might be also
      be used to bypass security checks that other APIs would provide.  For example:

      * Attackers used the WebUSB API to access others sites' crendentials on a hardware security,
      bypassing same-origin checks in an early U2F API.  [[YUBIKEY-ATTACK]]

  <h3 id="native-ui">
    Does this specification allow an origin some measure of control over a user
    agent's native UI?
  </h3>

  This concerns interaction with the UI, such as modification of browser display, or even such things as
  displaying a common pop-up with origin-controlled input, such as Notifications.

  <h3 id="temporary-id">Does this specification expose temporary identifiers to the web?</h3>

  Some identifiers may be difficult to pin-point  but they revolve around information that is stable in short-term manner (seconds, minutes, days) and may be available to web origins, whether in cross-origin manner or even cross-browser manner.

  *  [BATTERY-STATUS-API] is possibly the most unexpected example when battery may act as a short-term identifier (see also <a href="https://eprint.iacr.org/2015/616">Leaking Battery</a>, <a href="https://blog.lukaszolejnik.com/battery-status-not-included-assessing-privacy-in-w3c-web-standards/">Battery Status Not Included</a>)
  *   Ambient Light readout may be perceived in a <a href="https://blog.lukaszolejnik.com/privacy-of-ambient-light-sensors/">similar manner</a>
  *   Information such as ConnectionType, ConnectionType and rtt exposed by [NETWORK-INFORMATION-API]

  <h3 id="first-third-party">Does this specification distinguish between behavior in first-party and third-party contexts?</h3>

  *   Section 2.1 of [[FIRST-PARTY-ONLY]] defines "first-party" in line with
      existing browser behavior (Chrome and Firefox).

  <h3 id="private-browsing">
    How does this specification work in the context of a user agent’s Private Browsing Modes mode?
  </h3>

  Some web features behave differently in private browsing modes (with respect to normal browsing modes).
  This is not always desirable. Features should work in such a way that the website would not be able to
  determine that the user was in a privacy browsing mode. This includes graceful degradation when a feature stops
  functioning in a way still not revealing the browsing mode. Some examples

  *   Ideally, the feature would work in such a way that the website would not
      be able to determine that the user was in private browsing mode.

  *   Less ideally, the feature wouldn't work, but the website still wouldn't
      be able to distinguish a private browsing context from simply being denied
      permission to use the feature (for instance).

  *   Unideally, the feature wouldn't exist at all in private browsing mode, which
      means that the user wouldn't be exposing data, but the website can probably
      tell that the user is in that state.

  Some examples:

  *  Non-standardised method <a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/requestFileSystem">requestFileSystem</a> enabling the <a href="https://gist.github.com/jherax/a81c8c132d09cc354a0e2cb911841ff1">detection</a> private browsing mode
  *  [PAYMENT-REQUEST-API] allowed the <a href="https://blog.lukaszolejnik.com/privacy-of-web-request-api/">detection</a> of “incognito” mode


  <h3 id="storage">
    Does this specification persist data to a user's local device?
  </h3>
  User agents are able to store persistent data to user’s devices.
  The simplest examples are browsing history, cookies, or localStorage.

  *   How should user agent's "Clear browsing data" functionality work with
      this data? Are there caches that the user agent needs to be particularly
      careful with?

  <h3 id="considerations">
    Does this specification have a "Security Considerations" and "Privacy
    Considerations" section?
  </h3>

  Documenting the various concerns and potential abuses in "Security Considerations" and "Privacy Considerations"
  sections of a document is a good way to help implementers and web developers understand the risks that a feature
   presents, and to ensure that adequate mitigations are in place.
  I
  f it seems like a feature does not have security or privacy impacts,
  then say so inline in the spec section for that feature:

  <blockquote>
    There are no known security or privacy impacts of this feature.
  </blockquote>

  Saying so explicitly in the specification serves several purposes:
  <ol>
    <li>
      Shows that a spec author/editor has explicitly considered security and
      privacy when designing a feature.
    </li>
    <li>
      Provides some sense of confidence that there might be no such impacts.
    </li>
    <li>
      Challenges security and privacy minded individuals to think of and find
      even the potential for such impacts.
    </li>
    <li>
      Demonstrates the spec author/editor's receptivity to feedback about such
      impacts.
    </li>
    <li>
      Demonstrates a desire that the specification should not be introducing security and privacy issues
    </li>
  </ol>

  When saying this, however, the crucial aspect is to actually considering security and privacy.
  All new specifications must have security and privacy considerations sections to be considered
  for wide reviews. Interesting features added to the web platform generally often already had security
  and/or privacy impacts.

  <h3 id="relaxed-sop">
    Does this specification allow downgrading default security characteristics?
  </h3>

  *   <code>document.domain</code>
  *   [[CORS]]
  *   [[WEBMESSAGING]]
  *   <code>referrer 'unsafe-always'</code>

  <h3 id="behavioral-monitoring">
    Does this specification allow the persistent monitoring of user behavior?
  </h3>
    Users on the web can be monitored in variaty of means, for example using access to sensors potentially
    providing mobility patterns (e.g. accelerator, light sensor, web bluetooth), or directly on the web such as
    the monitoring of mouse movement, which are biometric data conveying rich information.

</section>

<section>
  <h2 id="mitigations">
    Mitigation Strategies
  </h2>

  To mitigate the security and privacy risks you’ve identified in your
  specification as you’ve filled out the questionnaire and consulted with PING,
  you may want to apply one or more of the mitigations described below to your
  feature.

  <h3 id="data-minimization">
    Data Minimization
  </h3>

  Minimization is a strategy that involves exposing as little information to
  other communication partners as is required for a given operation to
  complete. More specifically, it requires not providing access to more
  information than was apparent in the user-mediated access or allowing the
  user some control over which information exactly is provided.

  For example, if the user has provided access to a given file, the object
  representing that should not make it possible to obtain information about
  that file's parent directory and its contents as that is clearly not what is
  expected.

  Basic fingerprinting guidance can be found here.

  In context of data minimization it is natural to ask what data is passed
  around between the different parties, how persistent the data items and
  identifiers are, and whether there are correlation possibilities between
  different protocol runs.

  For example, the W3C Device APIs Working Group has defined the following
  requirements in their Device API Privacy Requirements document.

  *   APIs must make it easy to request as little information as required for
      the intended usage. For instance, an API call should require specific
      parameters to be set to obtain more information, and should default to
      little or no information.
  *   APIs should make it possible for user agents to convey the breadth of
      information that the requester is asking for. For instance, if a developer
      only needs to access a specific field of a user address book, it should be
      possible to explicitly mark that field in the API call so that the user
      agent can inform the user that this single field of data will be shared.
  *   APIs should make it possible for user agents to let the user select,
      filter, and transform information before it is shared with the requester.
      The user agent can then act as a broker for trusted data, and will only
      transmit data to the requester that the user has explicitly allowed.

  Data minimization is applicable to specification authors, implementers as
  well as to those deploying the final service.

  As an example, consider mouse events. When a page is loaded, the application
  has no way of knowing whether a mouse is attached, what type of mouse it is
  (e.g., make and model), what kind of capabilities it exposes, how many are
  attached, and so on. Only when the user decides to use the mouse — presumably
  because it is required for interaction — does some of this information become
  available. And even then, only a minimum of information is exposed: you could
  not know whether it is a trackpad for instance, and the fact that it may have
  a right button is only exposed if it is used. For instance, the Gamepad API
  makes use of this data minisation capability. It is impossible for a Web game
  to know if the user agent has access to gamepads, how many there are, what
  their capabilities are, etc. It is simply assumed that if the user wishes to
  interact with the game through the gamepad then she will know when to action
  it — and actioning it will provide the application with all the information
  that it needs to operate (but no more than that).

  The way in which the functionality is supported for the mouse is simply by
  only providing information on the mouse's behaviour when certain events take
  place. The approach is therefore to expose event handling (e.g., triggering
  on click, move, button press) as the sole interface to the device.

  Two features that have minimized the data they make available are:
  * [BATTERY-STATUS-API] <em>“The user agent should not expose high
    precision readouts”</em>
  * [SENSORS-API] <em>“Limit maximum sampling frequency”, “Reduce accuracy”</em>


  <h3 id="privacy-friendly-defaults">
    Privacy Friendly Defaults
  </h3>

  Users often do not change defaults, as a result, it is important that the
  default mode of a specification minimizes the amount, identifiability, and
  persistence of the data and identifiers exposed.  This is particularly true
  if a protocol comes with flexible options so that it can be tailored to
  specific environments.

  <h3 id="user-mediation">
    Explicit user mediation
  </h3>

  If the security or privacy risk of a feature cannot otherwise be mitigated in
  a specification, optionally allowing an implementer to prompt a user may
  provide the best security/privacy possible.  If the specification does not
  allow for the implementer to prompt, it may result in divergence
  implementations by different user agents as some user agents choose to
  implement more privacy-preserving version.

  It is possible that the risk of a feature cannot be mitigated because the
  risk is endemic to the feature itself.  For instance, [[ GEOLOCATION-API ]]
  reveals a user’s location, and wouldn’t be particularly useful if it didn’t;
  user agents generally gate access to the feature on a permission prompt which
  the user may choose to accept.  This risk is also present and should be
  accounted for in features that expose personal data or identifiers.

  Designing such prompts is difficult as is determining the duration and timing
  that the permission should provide. Generally speaking, the duration and
  timing of the prompt should be inversely proportional to the risk posed by
  the data exposed.  Often, the best prompt is one that is clearly tied to a
  user action, like the file picker.  And, walls of text may not be read by
  users or understood by all.

  These prompts should also include considerations for what, if any, control a
  user has over their data after it has been shared with other parties.  For
  example, are users able to determine what information was shared with other
  parties?

  <h3 id="restrict-to-first-party">
    Explicitly restrict the feature to first party origins
  </h3>

  As described in the "Third-Party Tracking" section, the power of the web is
  the mixing of first and third party content in a single page, but, this
  introduces risk where the third party content can use the same set of web
  features as the first party content.  One way to mitigate this risk is for
  the specification to  restrict access to the feature to the first party
  origin only and make third party’s access to the feature an optional
  implementation for conformance.

  <h3 id="secure-contexts">
    Secure Contexts
  </h3>

  If the primary risk that you’ve identified in your specification is the
  threat posed by <a>active network attacker</a>, offering a feature to an
  insecure origin is the same as offering that feature to every origin because
  the attacker can inject frames and code at will. Requiring an encrypted and
  authenticated connection in order to use a feature can mitigate this kind of
  risk.

  However, requiring a secure context is not sufficient to mitigate many
  privacy risks or even security risks from other threat actors than active
  network attackers.

  <h3 id="drop-feature">
    Drop the feature
  </h3>

  The simplest way to mitigate potential negative security or privacy impacts
  of a feature is to drop the feature.
  Every feature in a spec should be considered guilty of harming security
  and/or privacy until proven otherwise.
  Discussing dropping the feature as a mitigation for security or privacy
  impacts is a helpful exercise as it helps illuminate the tradeoffs between
  the feature, whether it is exposing the minimum amount of data necessary,
  and other possibly mitigations.

  Every specification should seek to be as small as possible, even if only
  for the reasons of reducing and minimizing security/privacy attack surface(s).

  By doing so we can reduce the overall security and privacy attack surface
  of not only a particular feature, but of a module (related set of
  features), a specification, and the overall web platform.

  Examples

  *   <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1313580%20">Mozilla<a> and <a href="https://bugs.webkit.org/show_bug.cgi?id=164213">WebKit</a> dropped Battery Status API
  *   <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1359076">Mozilla dropped<a> devicelight, deviceproximity and userproximity events

  <h3 id="privacy-impact-assessment">
    Making a privacy impact assessment
  </h3>

  Some features are potentially supplying very sensitive data, and it is
  the end-developer, system owners, or managers responsibility to realize
  this and act accordingly in the design of his/her system. Some use may
  warrant conducting as privacy impact assessment, especially when data
  relating to individuals may be processed.

  Specifications that expose such sensitive data should include a
  recommendation that websites and applications adopting the API — but not
  necessarily the implementing user agent — conduct a privacy impact assessment
  of the data that they collect.

  A features that recommends such is:
  *   [GENERIC-SENSORS] advices to consider performing of a privacy impact
  assessment


</section>

<pre class="anchors">
urlPrefix: http://www.w3.org/TR/html5/
  type: interface
    urlPrefix: webappapis.html
      text: NavigatorPlugins
</pre>

<pre class="link-defaults">
spec:html5; type:element; text:script
</pre>

<pre class="biblio">
  "AMBIENT-LIGHT-API": {
    "href": "https://www.w3.org/TR/ambient-light/",
    "title" "Ambient Light API",
    "publisher": "W3C",
    "authors": ["Anssi Kostiainen"]
  },
{
  "BLUETOOTH": {
      "href": "https://webbluetoothcg.github.io/web-bluetooth/",
      "title": "Web Bluetooth",
      "publisher": "W3C",
      "authors": [ "Jeffrey Yasskin", "Vincent Scheib" ]
  },
  "BATTERY-STATUS": {
    "href": "https://www.w3.org/TR/battery-status/",
    "title" "Battery Status API",
    "publisher": "W3C",
    "authors": ["Anssi Kostiainen", "Mounir Lamouri"]
  },
  "COMCAST": {
      "href": "http://arstechnica.com/tech-policy/2014/09/why-comcasts-javascript-ad-injections-threaten-security-net-neutrality/",
      "title": "Comcast Wi-Fi serving self-promotional ads via JavaScript injection",
      "publisher": "Ars Technica",
      "authors": [ "David Kravets" ]
  },
  "CREDENTIAL-MANAGEMENT": {
      "href": "https://w3c.github.io/webappsec/specs/credentialmanagement/",
      "title": "Credential Management",
      "publisher": "W3C",
      "authors": [ "Mike West" ]
  },
  "DISCOVERY": {
      "href": "http://dvcs.w3.org/hg/dap/raw-file/tip/discovery-api/Overview.html",
      "title": "Network Service Discovery",
      "authors": [ "Rich Tibbett" ],
      "publisher": "W3C"
  },
  "GEOFENCING": {
      "href": "https://github.com/slightlyoff/Geofencing/blob/master/explainer.md",
      "title": "Geofencing Explained",
      "authors": [ "Alex Russell" ]
  },
  "HOMAKOV": {
      "href": "http://homakov.blogspot.de/2014/01/using-content-security-policy-for-evil.html",
      "title": "Using Content-Security-Policy for Evil",
      "authors": [ "Egor Homakov" ]
  },
  "VERIZON": {
      "href": "http://adage.com/article/digital/verizon-target-mobile-subscribers-ads/293356/",
      "title": "Verizon looks to target its mobile subscribers with ads",
      "publisher": "Advertising Age",
      "authors": [ "Mark Bergen", "Alex Kantrowitz" ]
  },
  "PAYMENT-REQUEST-API": {
    "href": "https://www.w3.org/TR/payment-request/",
    "title" "Payment Request",
    "publisher": "W3C",
    "authors": ["Adrian Bateman", "Zach Koch", "Roy McElmurry", "Domenic Denicola", "Marcos Cáceres"]
  },
  "PERMISSIONS": {
    "href": "https://www.w3.org/TR/permissions/",
    "title" "Permissions API",
    "publisher": "W3C",
    "authors": ["Mounir Lamouri", "Marcos Cáceres", "Jeffrey Yasskin"]
  },
  "PII": {
      "href": "https://en.wikipedia.org/wiki/Personally_identifiable_information",
      "title": "Personally identifiable information",
      "publisher": "Wikipedia"
  },
  "TIMING": {
      "href": "http://www.contextis.com/documents/2/Browser_Timing_Attacks.pdf",
      "title": "Pixel Perfect Timing Attacks with HTML5",
      "authors": [ "Paul Stone" ],
      "publisher": "Context Information Security"
  },
  "RFC6454": {
      "href": "https://tools.ietf.org/html/rfc6454",
      "title": "The Web Origin Concept",
      "authors": [ "Adam Barth" ],
      "publisher": "IETF"
  },
  "RFC7258": {
      "href": "http://tools.ietf.org/html/rfc7258",
      "title": "Pervasive Monitoring Is an Attack",
      "authors": [ "Stephen Farrell", "Hannes Tschofenig" ],
      "publisher": "IETF"
  },
  "FIRST-PARTY-ONLY": {
      "href": "https://tools.ietf.org/html/draft-west-first-party-cookies",
      "title": "'First-Party-Only' Cookies",
      "authors": [ "Mike West" ],
      "publisher": "IETF"
  },
  "YUBIKEY-ATTACK": {
      "href": "https://www.wired.com/story/chrome-yubikey-phishing-webusb/",
      "title": "Chrome Lets Hackers Phish Even 'Unphishable' YubiKey Users",
      "authors": [ "Andy Greenberg" ],
      "publisher": "Wired"
}
</pre>
